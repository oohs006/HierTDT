{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3691a064",
   "metadata": {},
   "source": [
    "## Part I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a228903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff58512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_jsonl(file_path, fields, cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached data from '{cache_path}'...\")\n",
    "        return pickle.load(open(cache_path, 'rb'))\n",
    "    print(f\"Reading '{file_path}' and extracting fields {fields}...\")\n",
    "    data = {field: [] for field in fields}\n",
    "    with open(file_path, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            for field in fields:\n",
    "                data[field].append(obj.get(field))\n",
    "    pickle.dump(data, open(cache_path, 'wb'))\n",
    "    print(f\"Cached extracted data to '{cache_path}'\")\n",
    "    return data\n",
    "\n",
    "\n",
    "train_cache = \"train_data.pkl\"\n",
    "train_fields = [\"text\", \"label\"]\n",
    "train_data = load_and_cache_jsonl(\"corpus.jsonl\", train_fields, train_cache)\n",
    "texts = train_data[\"text\"]\n",
    "labels = train_data[\"label\"]\n",
    "print(f\"Loaded {len(texts)} training documents.\")\n",
    "\n",
    "\n",
    "new_cache = \"new_data.pkl\"\n",
    "new_fields = [\"text\", \"da_label\", \"label\"]\n",
    "new_data = load_and_cache_jsonl(\"query.jsonl\", new_fields, new_cache)\n",
    "new_texts = new_data[\"text\"]\n",
    "new_da_labels = new_data[\"da_label\"]\n",
    "new_labels = new_data[\"label\"]\n",
    "print(f\"Loaded {len(new_texts)} new documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "vocab_cache = \"vocabulary.pkl\"\n",
    "\n",
    "if os.path.exists(vocab_cache):\n",
    "    with open(vocab_cache, 'rb') as f:\n",
    "        vocabulary = pickle.load(f)\n",
    "    print(f\"Loaded vocabulary ({len(vocabulary)} terms) from cache.\")\n",
    "else:\n",
    "    kw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\n",
    "    keyword_results = kw_model.extract_keywords(texts, stop_words='english')\n",
    "    vocabulary = list({term for doc_kw in keyword_results for term, _ in doc_kw})\n",
    "    with open(vocab_cache, 'wb') as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "    print(f\"Extracted and cached {len(vocabulary)} vocabulary terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "if device == \"cuda\":\n",
    "    embedding_model = embedding_model.to(device)\n",
    "\n",
    "def get_embeddings(texts, cache_path=None, batch_size=512):\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        print(f\"Loading embeddings from {cache_path}...\")\n",
    "        return np.load(cache_path) \n",
    "\n",
    "    print(f\"Computing embeddings for {len(texts)} texts...\")\n",
    "    embs = embedding_model.encode(\n",
    "        texts, batch_size=batch_size, show_progress_bar=True\n",
    "    )\n",
    "    if cache_path:\n",
    "        print(f\"Saving embeddings to {cache_path}...\")\n",
    "        np.save(cache_path, embs)\n",
    "    return embs\n",
    "\n",
    "embeddings_file = \"tdt_embeddings.npy\"\n",
    "train_embeddings = get_embeddings(texts, cache_path=embeddings_file, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_components=5,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=100,\n",
    "    min_samples=50,\n",
    "    gen_min_span_tree=True,\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    vocabulary=vocabulary,\n",
    "    stop_words='english',\n",
    "    min_df=20,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(\n",
    "    bm25_weighting=True,\n",
    "    reduce_frequent_words=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb55c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "print(\"Training BERTopic model...\")\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    verbose=True\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings=train_embeddings)\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a07be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_topics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topics, f)\n",
    "\n",
    "print(\"Saved topic assignments to 'train_topics.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e555c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info[['Topic', 'Count', 'Name']].to_csv(\"topic_info.csv\", index=False)\n",
    "print(f\"Generated {len(topic_info)-1} topics (excluding outliers).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\n",
    "    \"tdt_topic_model\",\n",
    "    serialization=\"safetensors\",\n",
    "    save_embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    save_ctfidf=True\n",
    ")\n",
    "print(\"Saved BERTopic model to 'tdt_topic_model'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa95918",
   "metadata": {},
   "source": [
    "## Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\"tdt_topic_model\")\n",
    "print(\"Loaded BERTopic model.\")\n",
    "\n",
    "train_data = pickle.load(open(\"train_data.pkl\", \"rb\"))\n",
    "train_texts = train_data[\"text\"]\n",
    "train_labels = train_data[\"label\"]\n",
    "\n",
    "new_data = pickle.load(open(\"new_data.pkl\", \"rb\"))\n",
    "new_texts = new_data[\"text\"]\n",
    "new_da_labels = new_data[\"da_label\"]\n",
    "new_labels = new_data[\"label\"]\n",
    "\n",
    "train_embeddings = np.load(\"tdt_embeddings.npy\")\n",
    "train_topics = pickle.load(open(\"train_topics.pkl\", \"rb\"))\n",
    "\n",
    "print(f\"Training set: {len(train_texts)} docs, \" f\"New set: {len(new_texts)} docs\")\n",
    "print(f\"train_embeddings.shape = {train_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "new_embeddings = get_embeddings(new_texts, cache_path=None, batch_size=512)\n",
    "new_topics, new_probs = topic_model.transform(new_texts, embeddings=new_embeddings)\n",
    "print(\"Predicted topics for new documents.\")\n",
    "print(f\"new_embeddings.shape = {new_embeddings.shape}\")\n",
    "\n",
    "sim_matrix = cosine_similarity(new_embeddings, train_embeddings)\n",
    "print(f\"Computed similarity matrix of shape {sim_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92627db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(train_texts) =\", len(train_texts))\n",
    "print(\"len(new_texts)   =\", len(new_texts))\n",
    "print(\"train_embeddings.shape =\", train_embeddings.shape)\n",
    "print(\"new_embeddings.shape   =\", new_embeddings.shape)\n",
    "print(\"sim_matrix.shape       =\", sim_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdab9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()[[\"Topic\", \"Name\"]]\n",
    "topic_to_name = {\n",
    "    int(r.Topic): r.Name\n",
    "    for r in topic_info.itertuples(index=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522befb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "TOP_K = 10\n",
    "\n",
    "for i, (topic, prob) in enumerate(zip(new_topics, new_probs)):\n",
    "    rec = {\n",
    "        \"id\": i,\n",
    "        \"theme_name\": None,\n",
    "        \"theme_prob\": float(prob),\n",
    "        \"da_label\": new_da_labels[i],\n",
    "        \"label\": new_labels[i],\n",
    "        \"text\": new_texts[i],\n",
    "        \"top_10\": []\n",
    "    }\n",
    "    if topic == -1:\n",
    "        rec[\"theme_name\"] = \"Outlier\"\n",
    "        results.append(rec)\n",
    "        continue\n",
    "\n",
    "    rec[\"theme_name\"] = topic_to_name.get(int(topic), str(topic))\n",
    "    same_idxs = np.where(train_topics == topic)[0]\n",
    "    if same_idxs.size:\n",
    "        sims = sim_matrix[i, same_idxs]\n",
    "        topk_idxs = same_idxs[np.argsort(sims)[-TOP_K:]][::-1]\n",
    "        for idx in topk_idxs:\n",
    "            rec[\"top_10\"].append({\n",
    "                \"train_doc_id\": int(idx),\n",
    "                \"similarity\": float(sim_matrix[i, idx]),\n",
    "                \"label\": train_labels[idx],\n",
    "                \"text\": train_texts[idx]\n",
    "            })\n",
    "    results.append(rec)\n",
    "\n",
    "print(f\"Built top-{TOP_K} recommendations for {len(results)} new docs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new2train_top10_pretty.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "with open(\"new2train_pairs.jsonl\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in results:\n",
    "        for top in rec[\"top_10\"]:\n",
    "            out = {\n",
    "                \"da_label\": rec[\"da_label\"],\n",
    "                \"score\": top[\"similarity\"],\n",
    "                \"label\": top[\"label\"],\n",
    "                \"text\": top[\"text\"]\n",
    "            }\n",
    "            fout.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved 'new2train_top10_pretty.json' and 'new2train_pairs.jsonl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.8\n",
    "kept = 0\n",
    "with open(\"new2train_pairs.jsonl\", \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(\"new2train_filtered.jsonl\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        obj = json.loads(line)\n",
    "        if obj[\"score\"] > THRESHOLD:\n",
    "            fout.write(line)\n",
    "            kept += 1\n",
    "\n",
    "print(f\"Filtered pairs with score > {THRESHOLD}: kept {kept} records.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
